---
title: "Econometrics 2B: Time Series Analysis - Homework"
author: "Tim Kühleis, Bartolomeo Perazzoli"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup_and_data, include=FALSE}

# Setting up the Homework

# Load necessary Packages 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyr, fredr, dplyr, ggplot2, patchwork, urca, tseries, zoo, vars, forecast, patchwork, Metrics, gridExtra)

# Retrieve the Data from FRED using their API ----
### (You need an API Key to retrieve the Data, feel free to use mine for this 

fredr_set_key("8af1b2f98eef41dfe4b5ff6ffce1bbb0")

# Define sample window
start_date <- as.Date("2000-01-01")
end_date   <- as.Date("2019-12-31")

# 1. Germany industrial activity index (monthly, Seasonally Adjusted)
industrial <- fredr(
  series_id         = "DEUPROINDMISMEI",
  observation_start = start_date,
  observation_end   = end_date,
  frequency         = "m"
)

# 2. Retrieve BAML High-Yield OAS (aggregated to monthly avg., no SA needed)
credit_spread <- fredr(
  series_id           = "BAMLHE00EHYIOAS",
  observation_start   = start_date,
  observation_end     = end_date,
  frequency            = "m",       # monthly frequency
  aggregation_method   = "avg"      # average of daily values
)

# 3. Germany CPI (monthly, Seasonally ajusted data)
## Originally in our Mail we used the non SA data but SA-adjusted was available too so we switched 
cpi <- fredr(
  series_id         = "DEUCPALTT01IXOBSAM",
  observation_start = start_date,
  observation_end   = end_date,
  frequency         = "m"
) 

# Inspecting data briefly
head(industrial)
head(cpi)
head(credit_spread)
```

# Exercise 1 - Univariate Analysis

## Exercise 1.1: Plot each time-series in levels before applying any transformations

```{r exercise_1/1_plots, include=FALSE}
# Combine data into long format
data_raw <- bind_rows(
  credit_spread %>% 
    dplyr::select(date, value) %>%  # Prev. Problems with select fixed, explictly tell it to use select from dplyr
    mutate(series    = "Credit Spread",
           full_name = "ICE BofA Euro High Yield Index Option-Adjusted Spread"),
  industrial %>% 
    dplyr::select(date, value) %>% 
    mutate(series    = "Industrial Production",
           full_name = "Germany: Industrial Production Volume (Excl. Construction)"),
  cpi %>% 
    dplyr::select(date, value) %>% 
    mutate(series    = "Consumer Price Index",
           full_name = "Germany: Harmonized Consumer Price Index (HICP, All Items)")
)

# Set color palette 
series_colors <- c(
  "Credit Spread"        = "#1f77b4", 
  "Industrial Production" = "#ff7f0e", 
  "Consumer Price Index"  = "#2ca02c" 
)
# Create a custom template for plots ----
theme_academic_individual <- function(base_size = 12) {
  theme_bw(base_size = base_size) + 
    theme(
      plot.title      = element_text(face = "bold", size = rel(1.2), hjust = 0.5),
      plot.subtitle   = element_text(size = rel(0.9), hjust = 0.5, margin = margin(b = 10)),
      axis.title.x    = element_text(size = rel(1.0), margin = margin(t = 10)),
      axis.title.y    = element_text(size = rel(1.0), margin = margin(r = 10)),
      axis.text       = element_text(size = rel(0.9)),
      panel.grid.minor= element_blank(),
      panel.grid.major= element_line(color = "grey85", linewidth = 0.4),
      plot.caption    = element_text(size = rel(0.8), hjust = 0, margin = margin(t = 10)),
      plot.margin     = margin(10, 15, 10, 10) # t, r, b, l
    )
}


# Function to create nice axisbreaks for every plot automatically ---
create_nice_breaks <- function(min_val, max_val, n = 5) {
  pretty(c(min_val, max_val), n)
}

# Create and save individual plots ----

# Plot 1: Credit Spread
data_credit <- data_raw %>% filter(series == "Credit Spread")
plot_credit_spread <- ggplot(data_credit, aes(x = date, y = value)) +
  geom_line(color = series_colors["Credit Spread"], linewidth = 0.8) +
  scale_x_date(
    date_breaks = "2 years",
    date_labels = "%Y",
    limits      = c(min(data_raw$date), max(data_raw$date)), # Consistent x-axis range
    expand      = c(0.01, 0.01)
  ) +
  scale_y_continuous(
    name   = "Credit Spread (Percentage Points)",
    breaks = create_nice_breaks(min(data_credit$value), max(data_credit$value), n = 6) # More breaks
  ) +
  labs(
    title   = "ICE BofA Euro High Yield Index Option-Adjusted Spread (2000-2019)", # More focused title
    x       = "Date",
    caption = "Source: ICE Data Indices via FRED (BAMLHE00EHYIOAS)"
  ) +
  theme_academic_individual()

print(plot_credit_spread)

# Save Credit Spread Plot 
ggsave("credit_spread_plot.pdf", plot_credit_spread, width = 7, height = 5)


# Plot 2: Industrial Production
data_industrial <- data_raw %>% filter(series == "Industrial Production")
plot_industrial_production <- ggplot(data_industrial, aes(x = date, y = value)) +
  geom_line(color = series_colors["Industrial Production"], linewidth = 0.8) +
  scale_x_date(
    date_breaks = "2 years",
    date_labels = "%Y",
    limits      = c(min(data_raw$date), max(data_raw$date)),
    expand      = c(0.01, 0.01)
  ) +
  scale_y_continuous(
    name   = "Index 2015=100",
    breaks = create_nice_breaks(min(data_industrial$value), max(data_industrial$value), n = 6)
  ) +
  labs(
    title   = "Germany: Industrial Production Volume (excl. construction, 2000-2019)",
    x       = "Date",
    caption = "Source: OECD via FRED (DEUPROINDMISMEI)"
  ) +
  theme_academic_individual()

print(plot_industrial_production)

# Save Credit Spread plot 
ggsave("industrial_production_plot.pdf", plot_industrial_production, width = 7, height = 5)


# Plot 3: Consumer Price Index
data_cpi <- data_raw %>% filter(series == "Consumer Price Index")
plot_cpi <- ggplot(data_cpi, aes(x = date, y = value)) +
  geom_line(color = series_colors["Consumer Price Index"], linewidth = 0.8) +
  scale_x_date(
    date_breaks = "2 years",
    date_labels = "%Y",
    limits      = c(min(data_raw$date), max(data_raw$date)),
    expand      = c(0.01, 0.01)
  ) +
  scale_y_continuous(
    name   = "Germany: Harmonized CPI (Index 2015=100)",
    breaks = create_nice_breaks(min(data_cpi$value), max(data_cpi$value), n = 6)
  ) +
  labs(
    title   = "Germany: Harmonized CPI (All items, 2000-2019)",
    x       = "Date",
    caption = "Source: OECD via FRED (DEUCPIALLMINMEI)"
  ) +
  theme_academic_individual()

print(plot_cpi)

# Save CPI Plot 
ggsave("cpi_plot.pdf", plot_cpi, width = 7, height = 5)

# Creating a Combined Plot 
combined_plot <- grid.arrange(plot_credit_spread, plot_industrial_production, plot_cpi, ncol = 1)
ggsave("exercise_1_combined_plot.pdf", combined_plot, width = 8, height = 12)

```

```{r exercise_1_graphcis, results='asis'}
knitr::include_graphics("exercise_1_combined_plot.pdf")
```

### Comment on these time series Plots

**Credit Spread Plot**

Before turning to the first plot, we briefly introduce the nature and construction of this time series, as it is less conventional than the other two, which most Economists should be familiar with. The ICE BofA Euro High Yield Index Option-Adjusted Spread (OAS) measures the additional yield that investors require, over the euro area risk-free rate, to hold euro-denominated corporate bonds rated below investment grade (i.e., lower than BB by Fitch or Ba1 by Moody’s). Only publicly issued bonds with an outstanding volume above €100 million from issuers domiciled in countries with investment-grade sovereign ratings are included. The spread is adjusted for embedded options and aggregated using market-capitalization weighting. In "less technical" terms, the series captures the risk premium associated with holding Eurozone "junk" bonds, reflecting both credit and liquidity risk.

The plot shows that the credit spread generally fluctuates within a 3–5 percentage point band during stable periods. Sharp spikes in the spread coincide with major episodes of financial turmoil. The elevated spread in 2001–2003, peaking around 15%, reflects heightened risk perceptions during the burst of the "dot-com-bubble" and the post-9/11 recession. The most pronounced increase occurs during the global financial crisis (2007–2009), with a peak above 20% following the collapse of the US-Bank Lehman Brothers in September 2008. The final surge in our time series between 2011 and 2013 reflects the European sovereign debt crisis.

However, it is important to note that the index only includes high-yield corporate bonds issued by firms domiciled in countries with investment-grade sovereign ratings, such as Germany. As a result, it does not fully capture the sharp rise in credit spreads for corporations based in fiscally less stable countries like Greece and Portugal, both of which lost investment-grade status in 2010. The observed increase in spreads during the European debt crisis thus primarily reflects broader market risk aversion and heightened perceptions of credit risk among issuers in core Eurozone economies—particularly those expected to act as lenders of last resort. For our analysis this is exactly what we want to focus on.

For our time series analysis, visually inspecting the series we see the series exhibits strong mean-reversion around a constant level and no visible trend, suggesting weak stationarity.

**Industrial Production Index**

The second plot shows Germany's industrial production index (excluding construction) from 2000 to 2019, expressed in real terms with 2015 as the base year. We can divide the evolution of the series into several key phases. From 2003 to mid-2008, the index displays a steady and almost linear upward trend, reflecting sustained industrial growth. This is followed by a sharp contraction of over 20% during the global financial crisis. The recovery begins in early 2009 but remains gradual and volatile. Pre-crisis output levels are only reached again by late 2015, an astounding 7 years after the GFC began. After 2017, the series begins to decline again, with little evidence of structural growth, indicating a potential stagnation of the German industrial sector in the face of increasing global uncertainty.

This time series effectively captures the cyclical and structural dynamics of Germany’s export-oriented manufacturing economy. The dramatic collapse in 2008–2009 underscores Germany's exposure to global trade disruptions during the financial crisis. The sluggish recovery and cyclical fluctuations reflect subsequent headwinds, including the euro area sovereign debt crisis (2011–2012) and the China-led global slowdown in 2015—particularly relevant given China’s role as a major importer of German industrial goods. The post-2017 downturn likely stems from declining external demand and rising global trade tensions.

Visual inspection suggests that the series is clearly non-stationary, characterized by both a deterministic trend and a shift in level post GFC. To obtain a stationary series, we take the first difference of the outcome. Additionally, we will use a logarithmic transformation as log-differences can be interpreted as month-over-month growth rates in industrial production.

**Consumer Price Index**

The third plot show the harmonized consumer price index (HICP) for Germany from 2000 to 2019, with 2015 as the base year. Over the entire sample period, the series exhibits a smooth and almost linear upward trend, increasing from approximately 80 to 105. Assuming linearity, this corresponds to an average annual growth rate of roughly 1.25 percentage points, which is consistent with the broader macroeconomic environment of low and stable inflation during this time period. Notably, the sample ends just before two major inflationary shocks: the COVID-19 pandemic and the 2022 energy crisis triggered by Russia’s invasion of Ukraine—both of which led to significant price pressures in subsequent years.

Throughout the period shown, the index displays no signs of mean reversion, and inflation appears largely unaffected by the external shocks visible in the other two series. Based on visual inspection, the series clearly exhibits a deterministic trend component. Thus, it may still be trend-stationary. To determine whether the process is stationary or not, we will conduct formal unit root tests in the next exercise, which will provide more robust evidence regarding the stationarity properties of the series. If we conclude, at least as a first step, that the series is non-stationary, we will apply log differencing, consistent with our approach to the IPI series. This transformation may yield a stationary process and additionally offers a smooth interpretation as month-over-month inflation rates.

## Exercise 1.2: Conduct unit root and stationarity tests for each series

Given our previous results we propose to test the following unit root specifications:

1)  We will log CPI and IPI for two reasons, first trends here have a multiplicative relationship and not a linear one hence logarithm allows us to create a linear relationship which is necessary to apply AR, ARMA etc. (Linear Time Series Analysis to them)

2)  Given that we already know we will apply at least 1 unit differences to get stationary results I(1) processes from our previous visual inspection of the data this gives us a nice feature for interpretation (e.g. we can interprete our log differences as growth rates *GIVE REASON HERE BRIEFLY*)

```{r exercise_1/2, include=FALSE}

## Exercise 1.2 Unit Root and Stationarity tests 

# 1. Take Logs and deltas of time series ----

industrial <- industrial %>% 
  mutate(log_value = log(value))  %>% 
  mutate(dlog_value = log_value - lag(log_value)) %>%
  drop_na(dlog_value) # Drop first observation because of NA (othwerwise VARselect won't work)



cpi <- cpi %>% 
  mutate(log_value = log(value))  %>% 
  mutate(dlog_value = log_value - lag(log_value)) %>%
  drop_na(dlog_value) # Drop first observation because of NA (othwerwise VARselect won't work)

# Take FD for credit spreads --> No Log Transformation here 
credit_spread <- credit_spread %>%
  mutate(cs_d1= credit_spread$value - lag(credit_spread$value)) %>%
  drop_na(cs_d1)

# 1.2 Briefly look at plots of the Transformed Data 

plot(industrial$dlog_value) # no visible trend, fluctuates around constant mean close to 0 --> DRIFT 
plot(cpi$dlog_value) # more more volatile, no visible trend, fluctuates around constant around mean a bit > 0  --> DRIFT
plot(credit_spread$value) # fluctuation around constant mean 4-6 bps with a few dramatic spikes in crises 
plot(credit_spread$cs_d1) # look stationary around 0, with crisis outliers 

# 2. Unit Root tests ----

# 2.1 Use the VARS function to check the optimal # of lags ---- 
## Reminder: We are using monthly data so reasonable maximum lag order for monthly data is 12 months
# Use AIC for lag selection in ADF tests (Ng & Perron 2001 Lag Length Selection and the Construction of Unit Root Tests with Good Size and Power) 
# AIC better size and power for unit root tests 

VARselect(industrial$log_value, lag.max=12) #Result: AIC: 8 --> choose 7
VARselect(cpi$log_value, lag.max=12) #AIC 12 
VARselect(industrial$dlog_value, lag.max=12) #Result: AIC: 7 --> choose 7
VARselect(cpi$dlog_value, lag.max=12) #Result: consensus 12 lags 
VARselect(credit_spread$value, lag.max=12) # Result: AIC: 7  --> choose 7 Lags 
VARselect(credit_spread$cs_d1, lag.max = 12) # AIC: 6


## 2.1. Unit Root tests without Differentiation ---- 

### URCA - for unit root tests (w/o trends in the beginning only add trend if phi3: H0(trend=0)  —> reject)
### Remember use an appropriate maximum number of lags --> 12 for monthly data 
### We selected AIC lags anyway so we can let the function do this automatically and just give max # lags (12 for monthly)

#### a) Industrial Production (log‐level) with trend
adf_ip_lvl <- ur.df(industrial$log_value,
                    type       = "trend",
                    lags       = 12,
                    selectlags = "AIC")
summary(adf_ip_lvl)

pp_ip_lvl  <- ur.pp(industrial$log_value,
                    type   = "Z-tau", # trend
                    model  = "trend",
                    use.lag = 8)
summary(pp_ip_lvl)

kpss_ip_lvl <- kpss.test(industrial$log_value, null="Trend") # H0: trend stationarity 
print(kpss_ip_lvl)


#### b) CPI (log‐level) with trend
adf_cpi_lvl <- ur.df(cpi$log_value,
                     type       = "trend",
                     lags       = 12,
                     selectlags = "AIC")
summary(adf_cpi_lvl)

pp_cpi_lvl  <- ur.pp(cpi$log_value,
                     type   = "Z-tau", #trend
                     model  = "trend",
                     use.lag = 8)
summary(pp_cpi_lvl)

kpss_cpi_lvl <- kpss.test(cpi$log_value, null="Trend") #H0: trend stationarity
print(kpss_cpi_lvl)


#### c) Credit Spread (level) with drift
adf_cs_lvl <- ur.df(credit_spread$value,
                    type       = "drift",
                    lags       = 12,
                    selectlags = "AIC")
summary(adf_cs_lvl)

pp_cs_lvl  <- ur.pp(credit_spread$value,
                    type   = "Z-alpha", #no trend 
                    model  = "constant",
                    use.lag = 7)
summary(pp_cs_lvl)

kpss_cs_lvl <- kpss.test(credit_spread$value, null="Level") #H0: level stationarity 
print(kpss_cs_lvl)


# Only for Credit Spread run a Zivot–Andrews test too: allow one break in the intercept (AO specification)
## Credit Spread seems to have a structural break in 2009/2009 financial crisis 
za_cs_lvl <- ur.za(credit_spread$value,
                   model = "intercept",  # break in level
                   lag   = 7)        

summary(za_cs_lvl)

## Result not 100% safe --> we will continue working with first differences 



## 2.2 Unit Root tests with first order Differentiation for CPI and IPI ---- 
### Couldn't conclude Stationarity for any of the time series --> will now test FD for stationarrity


### a) FD log Industrial Production

adf_ip_diff <- ur.df(industrial$dlog_value,
                     type       = "drift",
                     lags       = 12,
                     selectlags = "AIC")
summary(adf_ip_diff)

pp_ip_diff  <- ur.pp(industrial$dlog_value,
                     type   = "Z-alpha",
                     model  = "constant",
                     use.lag = 7 )
summary(pp_ip_diff)

kpss_ip_diff <- kpss.test(industrial$dlog_value,
                          null = "Level")
print(kpss_ip_diff)


#### b) FD log CPI

adf_cpi_diff <- ur.df(cpi$dlog_value,
                      type       = "drift",
                      lags       = 12,
                      selectlags = "AIC")
summary(adf_cpi_diff)


pp_cpi_diff  <- ur.pp(cpi$dlog_value,
                      type   = "Z-alpha",
                      model  = "constant",
                      use.lag = 12)
summary(pp_cpi_diff)


kpss_cpi_diff <- kpss.test(cpi$dlog_value,
                           null = "Level")
print(kpss_cpi_diff)


#### c) FD levels difference Credit Spread 

adf_cs_diff <- ur.df(credit_spread$cs_d1,
                     type       = "drift",
                     lags       = 12,
                     selectlags = "AIC")
summary(adf_cs_diff)

pp_cs_diff  <- ur.pp(credit_spread$cs_d1,
                     type   = "Z-alpha",
                     model  = "constant",
                     use.lag = 6)
summary(pp_cs_diff)

kpss_cs_diff <- kpss.test(credit_spread$cs_d1,
                          null = "Level")
print(kpss_cs_diff)

```

*Result*: **Include 2 Tables here showing all the results of the manny tets in a nice overview, to many tests to create an extra table for all the tests**

-   Alle 3 Units fail stationarity in the first test which comes a bit suprising for credit spread because we originally would have thaugt that the Credit spread at least would be stationary without transformation --\> here we couldn't clearly conclude stationary or non-stationarity, the safe choce is always to go with non-stationarity (because stationarity are simplifying properties we can not just assume --\> easier to transform to stationary series)
-   First Differences passed all stationarity tests --\> All 3 series are integrated of order 1 --\> Can now continue to Model the ARMA process for these I(1) series

## Exercise 1.3: Identify and estimate candidate AR(p) and ARMA(p,q) models using the ACF and PACF. Which

model would you choose according to information criteria? Comment the results

```{r exercise_1/3, include=FALSE}

# Exercise 1.3 Identify and estimate candidate AR(p) and ARMA(p,q) models using ACF and PACF 
## Since we already saw that the we need to take First Differences for stationarity use the I(1) of the series for ACF and PACF Plots

## 1. Create ACF and PACF Plots for our differentiated series (visual inspection) ---- 

### A. Industrial (Dif log IPI)
p_ipi_acf  <- ggAcf(industrial$dlog_value, lag.max = 20) + labs(title="ACF: Δ log(Industrial Production Index)")
p_ipi_pacf <- ggPacf(industrial$dlog_value, lag.max = 20) + labs(title="PACF: Δ log(Industrial Production Index)")
(ipi_acf <- p_ipi_acf / p_ipi_pacf) + plot_layout(guides="collect")

#### B. CPI (Dif log CPI)
p_cpi_acf  <- ggAcf(cpi$dlog_value, lag.max = 20) + labs(title="ACF: Δ log(CPI)")
p_cpi_pacf <- ggPacf(cpi$dlog_value, lag.max = 20) + labs(title="PACF: Δ log(CPI)")
(cpi_acf <- p_cpi_acf / p_cpi_pacf) + plot_layout(guides="collect")

### C. Credit Spread (Dif CS)
p_cs_acf  <- ggAcf(credit_spread$cs_d1, lag.max = 20) + labs(title="ACF: Δ Credit Spread")
p_cs_pacf <- ggPacf(credit_spread$cs_d1, lag.max = 20) + labs(title="PACF: Δ Credit Spread")
(cs_acf <- p_cs_acf / p_cs_pacf) + plot_layout(guides="collect")

#### Results
# IPI: AR(3)
# CPI: Looks almost like White Noise > at most MA(1) if Ljung Box test fails
# CS: AR(1) or AR(6) because of the significant downward spike of PACF at lag 6 

## 1) Gather the three stationary series in a named list --------------
ts_list <- list(
  industrial     = industrial$dlog_value,
  core_infl      = cpi$dlog_value,
  credit_spread  = credit_spread$cs_d1
)

## 2) Helper: exhaustive (p,q) grid, return top-N by AIC & BIC ---------
get_top_models <- function(y, max_p = 7, max_q = 7, n = 5) {
  grid <- expand.grid(p = 0:max_p, q = 0:max_q)
  grid$AIC <- grid$BIC <- NA_real_
  
  for (i in seq_len(nrow(grid))) {
    # wrap in try() so a non-invertible candidate doesn’t kill the loop
    fit_try <- try(
      Arima(y,
            order            = c(grid$p[i], 0, grid$q[i]),
            include.constant = TRUE,          # OK even after differencing
            method           = "ML"),
      silent = TRUE)
    
    if (!inherits(fit_try, "try-error")) {
      grid$AIC[i] <- fit_try$aic
      grid$BIC[i] <- fit_try$bic
    }
  }
  
  list(
    topBIC = head(grid[order(grid$BIC), ], n),
    topAIC = head(grid[order(grid$AIC), ], n)
  )
}

## 3) Apply to each series; store top-5 tables + BIC-winner fit ---------
all_models <- lapply(ts_list, function(y) {
  tops <- get_top_models(y)
  best <- tops$topBIC[1, ]
  
  best_fit <- Arima(y,
                    order            = c(best$p, 0, best$q),
                    include.constant = TRUE,
                    method           = "ML")
  
  list(
    topBIC  = tops$topBIC,
    topAIC  = tops$topAIC,
    fit_obj = best_fit
  )
})

## 4) Console output – top 5 by BIC and AIC -----------------------------
for (nm in names(all_models)) {
  cat("\n=== Series:", nm, "===\n")
  cat("Top 5 by BIC:\n")
  print(all_models[[nm]]$topBIC, row.names = FALSE)
  cat("\nTop 5 by AIC:\n")
  print(all_models[[nm]]$topAIC, row.names = FALSE)
}

## 5) Overlay plot of observed vs. in-sample BIC-winner -----------------
for (nm in names(all_models)) {
  fit <- all_models[[nm]]$fit_obj
  y   <- ts_list[[nm]]
  
  ts.plot(y, fitted(fit),
          gpars = list(col = c("black", "red"),
                       lty = 1,
                       lwd = 2),
          main = sprintf("%s: ARMA(%d,%d) in-sample fit",
                         nm, fit$arma[1], fit$arma[2]),
          ylab = expression(Delta*log~value),
          xlab = "Time")
  
  legend("topleft",
         legend = c("Observed", "Fitted"),
         col    = c("black", "red"),
         lty    = 1,
         lwd    = 2,
         bty    = "n")
}

## D. Extra Approach: Use algorithmic search for ARIMA Process (Hyndman-Khandakar algorithm)
### run auto.arima on each
auto_models <- lapply(ts_list, function(y) {
  auto_fit <- auto.arima(y, seasonal = FALSE, ic = "bic",
                         stepwise = TRUE # Turned on for speed as this is a mere Bonus application
                         )
  
### capture summary info
  info <- list(
    model = auto_fit,
    p     = auto_fit$arma[1],
    q     = auto_fit$arma[2],
    AIC   = auto_fit$aic,
    BIC   = auto_fit$bic
  )
  info
})

### print out results
for (nm in names(auto_models)) {
  m <- auto_models[[nm]]
  cat("\n---", toupper(nm), "---\n")
  cat(sprintf("auto.arima chose ARMA(%d,%d)\n", m$p, m$q))
  cat(sprintf("   AIC = %.3f,   BIC = %.3f\n", m$AIC, m$BIC))
  print(summary(m$model))
}

```

## Exercise 1.4: For one of the series, plot the in-sample and out-of-sample forecasts from your best model. Comment the results

```{r exercise_1/4, include = FALSE}
# Exercise 1.4 Plot the in-sample and out-of-sample forecasts from your best model for one of the series
## We will choose Industrial Production Index (Best Model determined: MA(3) process)

y_full <- industrial$dlog_value            


# Step 1. Split our sample into "In-sample" and "Out-of-Sample" Data 
## We will use 80% of our sample for fitting the Model "in-sample" and the remaining "20%" are out-of-sample

n <- length(y_full)
split_idx <- floor(0.80 * n)               # 80 % train, 20 % test
train     <- window(y_full, end   = time(y_full)[split_idx])
test      <- window(y_full, start = time(y_full)[split_idx + 1])

# Step 2. Fit the MA(3) Model on the "training" data 
## Default Method: Start with CSS and find starting values then pin down exact values using MLE
## Both good theorethical properties but also computationally efficient 
best_fit <- Arima(train, order = c(0, 0, 3),
                  include.constant = TRUE) 


## 5. One-step-ahead fitted & h-step forecasts --------------------------
h  <- length(test) # 48 months
fc <- forecast(best_fit, h = h)            # point + 80/95 % intervals

## 6. Plot: training, test, fitted, forecast ----------------------------

# Extract time properties
ts_start <- start(y_full)
ts_freq  <- frequency(y_full)

# Align fitted values and forecast outputs to correct time index
fitted_ts   <- ts(fitted(best_fit), start = ts_start, frequency = ts_freq)
fc_mean_ts  <- ts(fc$mean,          start = time(test)[1], frequency = ts_freq)
fc_lower80  <- ts(fc$lower[,1],     start = time(test)[1], frequency = ts_freq)
fc_upper80  <- ts(fc$upper[,1],     start = time(test)[1], frequency = ts_freq)
fc_lower95  <- ts(fc$lower[,2],     start = time(test)[1], frequency = ts_freq)
fc_upper95  <- ts(fc$upper[,2],     start = time(test)[1], frequency = ts_freq)

# Open PDF device
pdf("in_sample_oos_forecast_plot.pdf", width = 10, height = 6)

# Plot styling
par(mar = c(4, 4.5, 3, 1), 
    mgp = c(2.5, 1, 0),
    cex.main = 1.2,
    cex.lab = 1.1,
    cex.axis = 1.0)

# Plot actual data
plot(y_full,
     col = "black", 
     lwd = 1.5,
     pch = 1,
     cex = 0.6,
     main = expression("Industrial " * Delta * "log IPI – In-Sample Fit & OOS Forecast"),
     xlab = "Time", 
     ylab = expression(Delta * "log industrial output"),
     ylim = range(y_full, fc_lower95, fc_upper95, na.rm = TRUE),
     las = 1)

# Add a subtle grid
grid(col = "lightgray", lty = "dotted", lwd = 0.5)

# Add fitted values
lines(fitted_ts, col = "red", lwd = 2.5)

# Add prediction intervals
lines(fc_upper95, col = "steelblue", lty = 2, lwd = 1.5)   # 95%
lines(fc_lower95, col = "steelblue", lty = 2, lwd = 1.5)
lines(fc_upper80, col = "darkblue",  lty = 3, lwd = 1.2)   # 80%
lines(fc_lower80, col = "darkblue",  lty = 3, lwd = 1.2)

# Add forecast mean
lines(fc_mean_ts, col = "blue", lwd = 2.5)

# Vertical line at forecast start
abline(v = time(test)[1], 
       col = "gray40", 
       lty = "dashed", 
       lwd = 1)

# Text annotation for forecast region
text(x = time(test)[1],
     y = max(y_full, na.rm = TRUE) * 0.9,
     labels = "Forecast", 
     pos = 4, 
     col = "gray40",
     cex = 0.9)

# Improved legend
legend("topright",
       legend = c("Actual", "Fitted (train)", "Forecast mean",
                  "80% PI", "95% PI"),
       col = c("black", "red", "blue", "darkblue", "steelblue"),
       lty = c(NA, 1, 1, 3, 2),
       lwd = c(NA, 2.5, 2.5, 1.2, 1.5),
       pch = c(1, NA, NA, NA, NA),
       pt.cex = c(0.6, NA, NA, NA, NA),
       bty = "n",
       cex = 0.9,
       inset = c(0.02, 0.02))

dev.off()


```

*Comment on the Results* Here somthing is very off --\> possible it is not good to choose an MA process here but should rather choose a Model that had an AR(p) process --\> let us look at identification again and see if we can find a better example

# Exercise 2 - : Multivariate analysis

```{r, include = FALSE}
rm(list=ls())
```

## Exercise 2.1: Select the optimal lag length for your multivariate model

## Exercise 2.2: Estimate the model and verify that the residuals satisfy standard properties
